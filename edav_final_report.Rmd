---
title: "A Visual Analysis of Data Breaches from 2005 to 2018"
author: "Michelle Chen, Yimin Wang, Jing Yi Zhou"
date: "December 8, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=12, fig.height=8)
```

### Introduction

In light of recent data breaches to major organizations such as Facebook and more recently Marriott, companies are beginning to take extra precautions to protect their data. Data breaches are not only costly to recuperate from, but also hurt businesses and consumers in the long-run. This movement sparks great discussion about the types of companies and industries that are typically targeted, when breaches occur, how the breaches are broadcasted to the public, and just how much of our data is at stake. Using United States data breaches information from 2005 to 2018, our group hopes to craft a comprehensive story centered on these breaches and help the public better understand why they are happening through meaningful visualizations and interactivity. 

Our team consisted of Michelle Chen, Yimin Wang, and Jingyi Zhou. The delegation of tasks was as follows: We all came together to come up with a coherent story to introduce novel insights using our visualizations. Michelle was in charge of the visualizations related to the number of records breached for specific categories, states and regions. These initial visualizations helped us to establish an understanding of the distribution of records breached across various states, categories, and regions. Yimin sought to analyze the breach type attribute in depth by analyzing the frequency of specific breach types across various industries and regions. Lastly, Jingyi plotted time-series data and visualizations related to the breach source. Report-wise, we each wrote our own respective sections related to the parts of the data we worked on. Jingyi Zhou consolidated everything into a nice shiny app for presentation purposes of our visualizations and analysis.  

Our ShinyApp is located here: https://jing-yi-zhou.shinyapps.io/shiny_app/

Our Github page is located here: https://github.com/jyzhou95/edav_final_project

OUR Executive summary is located here: 

### Description of data

We obtained our data from theDataMap, a non-profit organization focused on documenting all the places and entities that our personal data gets transferred to and from. theDataMap operates as a research project in the Data Privacy Lab, which is a program in the Institute for Quantitative Social Science at Harvard University. Interested in seeing what types of analysis and insight students and researchers could garner about data breaches, theDataMap is hosting a competition where they have released company breach data and is asking researchers to come up with novel insights from the data. Our team chose to utilize these datasets for our EDAV final project, which can be accessed here: https://thedatamap.org/contests/materials.php. We will also be submitting our final analysis to theDataMapT's competition. We hope that our exploratory data analysis can help promote greater awareness to the academic community and our fellow classmates at Columbia University regarding where and why data breaches occur.

We also supplemented our analysis with the most recent state population data from the US Census Bureau. 

### Analysis of data quality

The raw data comes in the form of four files. Note that we did not use every single . They are as follows: 

**orgsindex.csv** is a list of organizations and entities whose data sharing transaction(s) appear on theDataMap. The file has 5351 rows in total, not including the header row. The fields are as follows:

* *OrgID* - A unique identifier associated with the entity.

* *Name* - The name of the entity.

* *SourceType* - The type of source from which the name appeared.

**categories.csv** is a list of categories of data holders of health data. The file has 54 rows, not including the header row. These correspond to the nodes on the graph itself. The fields are as follows:

* *CatID* - The unique identifier associated with a node on the graph.

* *CatName* - The name that appears for the node.

* *Coordinates* - The 4 pixel coordinates that locate the box associated with the node.

* *Hover* - The descriptive text that appears when the mouse hovers over the box associated with the node.

**orgsindex.csv** is a list of organizations and entities whose data sharing transaction(s) appear on theDataMap. The file has 5351 rows in total, not including the header row. The fields are as follows:

* *OrgID* - is an association list of categories (CatID) from the categories file and organizations (OrgID) from the OrgsIndex file. The file has 5336 rows, not including the header row. The fields are:

* *Type* - The kind of source that provided the information, such as "Discharge" or "Breaches".

**prcbreaches2005-18.csv** contains a list of breaches associated with different categories and organizations. The file has 4126 rows, not 
including the header row. The fields are:

* *Records.Breached* - The total number of records breached for some of the organizations.
* *Records.Breached...Detail* - The number of records breached with more details about the breaches.
* *Name* - The name of the organization.
* *CatID* - The category ID used in the other data files.
* *Total.Records* - The total number of breaches for all of the organzations (more information than Records.Breached).
* *Region* - The region that the organization is located in.
* *Contact..etc.* - The contact information for the organization.
* *Category_dm* - The name of the category associated with the CatID.
* *Entity_prc* - The type of organization.
* *State* - The state that the organization is located in.
* *OrgID* - The Organization ID.
* *Location* - The city that the organization is located in.
* *Date.Made.Public* - The date that the breaches were made public.
* *Year* - The year that the breaches were made public.
* *Source.of.Breach.Notification* - The source that publicized the breaches.
* *Type* - The type of breach.
* *Description* - The description of the breach.
* *Example (HTML)* - The HTML code used to generate the list of examples for each category in the health visualization.

#### Merging the data together

We decided to merge the four original datasets together so that we could create one final, comprehensive data set with all of the important attributes that we wanted to analyze. First, we merged prc_breaches with catorgs using the a composite key made up of cat_id and org_id. Next, we merged this intermediary data set with the categories dataset on cat_id. Lastly, we merged this output with the orgsindex dataset on org_id. Our final dataset contained 4126 rows and 19 columns.



The columns are as follows:

* Org_id (ID) - The unique identifier for the organization

* Cat_id (ID) - The unique identifier for the category of organization

* Cat_name (Categorical) - The name of the category

* Records_breached (Continuous) - The number of records breached if there were records breached. NA if no records were breached.

* Records_breached_detail (String) - Description of the records breached. NA if no description. Some rows include total number of records maintained by the organization. Some rows include a note that no SSNs were breached. Some rows include miscellaneous information.

* Name (String) - The name of the organization

* State (Categorical) - The state that the organization is in

* Total_records (Continuous) - Total number of records maintained by the organization

* Category (Categorical) - Name of the category of the organization

* Entity_type (Categorical) - Code for the entity type
  + BSO - business other
  + BSF - business - financial and insurance services
  + BSR - business - retail and merchant
  + EDU - education institutions
  + GOV - government and military
  + MED - healthcare - medical provider
  + NGO - nonprofit organizations

* Breach_source (Categorical) - The source that published the breach

* Breach_type (Categorical) - The type of breach that occurred
  + DISC - Unintended disclosure
  + HACK - Hacking
  + CARD - Payment card fraud
  + INSD - Insider
  + PHYS - Physical loss
  + PORT - Portable device loss
  + STAT - Stationary device loss
  + UNKN - Cause of breach unknown

* Region (categorical) - The region that the company is in

* Breach_id (ID) - Unique identifier for the breach

```{r}
options(warn=-1)
suppressMessages(library(data.table))
suppressMessages(library(glue))
suppressMessages(library(vcd))
suppressMessages(library(tidyverse))
suppressMessages(library(data.table))
suppressMessages(library(ggplot2))
suppressMessages(library(lubridate))
suppressMessages(library(ggthemes))
suppressMessages(library(viridis))
suppressMessages(library(extracat))
suppressMessages(library(glue))
suppressMessages(library(ggmosaic))
  

parent_dir <- getwd()

dt.categories <- fread(glue("{parent_dir}/health_care_data/categories.csv"))
dt.catorgs <- fread(glue("{parent_dir}/health_care_data/catsorgs.csv"))
dt.orgsindex <- fread(glue("{parent_dir}/health_care_data/orgsindex.csv"))
dt.prcbreaches <- fread(glue("{parent_dir}/health_care_data/prcbreaches2005-18.csv"))

dt.categories_final <- dt.categories[,list(cat_id = CatID,
                                     cat_name = CatName)]

dt.catorgs_final <- dt.catorgs[,list(cat_id = CatID,
                               org_id = OrgID,
                               type = Type)]

dt.orgsindex_final <- dt.orgsindex[,list(org_id = OrgID,
                                   company_name = Name,
                                   source_type = SourceType)]

dt.prcbreaches_final <- dt.prcbreaches[,list(records_breached = Records.Breached,
                                       records_breached_detail = Records.Breached...Detail,
                                       name = Name,
                                       cat_id = CatID,
                                       total_records = Total.Records,
                                       region = Region,
                                       category = Category_dm,
                                       entity_type = Entity_prc,
                                       state = State,
                                       org_id = OrgID,
                                       location = Location,
                                       dt = Date.Made.Public,
                                       breach_source = Source.of.Breach.Notification,
                                       breach_type = Type)]

dt.orgsindex_final <- unique(dt.orgsindex_final, by = c("org_id"))

dt.master <- merge(merge(merge(dt.prcbreaches_final, dt.catorgs_final, by = c("cat_id", "org_id"), all.x = TRUE),
                   dt.categories_final, by = c("cat_id"), all.x = TRUE),
                   dt.orgsindex_final, by = c("org_id"), all.x = TRUE)

# Fix region
dt.region <- unique(dt.master[!is.na(region)][,list(region, state)])

# Merge it backc on
dt.master[,region := NULL]
dt.master <- merge(dt.master, dt.region, by = c("state"), all.x = TRUE)

# Add unique identifier to each breach
dt.master$breach_id <- 1:nrow(dt.master)

dt.date_format1 <- dt.master[grepl("\\-", dt)]
dt.date_format1$dt <- as.Date(dt.date_format1$dt, format = "%d-%b-%y")

dt.date_format2 <- dt.master[grepl("\\/", dt)]
dt.date_format2$dt <- as.Date(dt.date_format2$dt, format = "%m/%d/%Y")

dt.data <- rbind(dt.date_format1,
                  dt.date_format2)

dt.data[,dt := as.Date(dt, format = "%m/%d/%Y")]

dt.data_raw <- dt.data

  
# Rows where records breached is NA is assumed to be 0 
dt.data[is.na(records_breached) & total_records == 0]$records_breached <- 0
  
# Merge breach type name to data
dt.breach_type_name <- data.table(breach_type = c("DISC", "HACK", "CARD", "INSD", "PHYS", "PORT", "STAT", "UNKN"),
                                    breach_type_name = c("Unintended Disclosure",
                                                         "Hacking",
                                                         "Payment Card Fraud",
                                                         "Insider Fraud",
                                                         "Physical Loss",
                                                         "Portable Device Loss",
                                                         "Stationary Device Loss",
                                                         "Unknown"))
dt.data <- merge(dt.data, dt.breach_type_name, by = c("breach_type"))
  
# State population data retrieved from Wikipedia
dt.state_population <- fread(paste0(parent_dir, "/health_care_data/state_population.csv"))
dt.state_population[,V1 := NULL]
```


### Missing Data Analysis:

Before diving into our data visualizations, we wanted to examine the amount of missing data in our data set. We first used the mi package's missing_data.frame() function to obtain a visual of the amount of missing data for each observation/row number and a corresponding column variable. Red in the resulting chart means that there is very few or no missing data. We see that the majority of the data is present. However, the two columns, records_breached and records_breached_detail, presented a great deal of black, meaning that those cells/instances in the column have missing data. Since records_breached_detail is simply an expanded version of records_breached with additional qualitative details, from a numeric records breached perspective it is redundant and therefore the only column with a concerning amount of missing data is the records_breached continuous variable column. 

```{r}
suppressMessages(library(mi))
dt.data_raw$dt <- as.character(dt.data_raw$dt)
x<-suppressMessages(missing_data.frame(dt.data_raw))
suppressMessages(image(x))
```

To get a better look at various missing data patterns in our dataset, we used the library extracat and specifically the visna chart. We did a row sort on the visna chart to sort from most common to least common missing value combination. This type of visna chart is useful because we can see very easily which combinations of missing data values are most common as well as the relative frequencies of these missing data combinations. 

Looking at the vertical distributions, the only significant amount of missing data is from records_breached and records_breacthed_detail. Looking at the horizontal distributions, we can see that the most common missing value combination is records_breached and records_breached_detail. That makes sense because if there is no information about how many records were breached or if there were no records breached then there would be no additional detail that corresponds to it. We can also see that records_breached by itself is the second most frequent missing data value. At a similar level is no missing values at all. 

```{r}
visna(dt.data, sort = "r")
```

The missing_data.frame() and visna() chart functions gave us a good start to the analysis of our missing data. This seemed like an alarming amount of missing data from the records_ breached column. We compared the records_breached, records_breached_detail, and total records columns and found potential reasons for these missing values. The NA values did not necessarily mean that the data point was missing so to speak. We found that there were no instances where the value of records_breached was 0, which would weird if that were actually the case. We looked at the breakdown in relative frequency of the breach type between breaches where records_breached was NA versus when it wasn't. We found that there was many more instances of physical loss and unintended disclosures when records_breached was NA.  In addition, records_breached. We figured that this meant that in actuality, a lot of the NAs actually meant 0 records were breached. It would make sense for there to be no records breached in these cases, since it's unlikely that personal records what be what was lost or unintentionally disclosed. And only lastly and very infrequently are there actually missing values. Ultimately, this does not affect our analysis and exploration much because we are mainly looking at breach instances--it doesn't matter whether there is data on the number of records breached. For the analysis that we are doing on records_breached, it doesn't matter that much that there are a lot of 0s because we are looking at the distribution of number of records breached given that a breach occurred in the first place.

#### Mosaic Plot of Region and Breach Type

```{r}

dt.frequency_bar_plot <- dt.data[,list(records_breached, breach_type_name)]
dt.frequency_bar_plot[,is_records_breached_na := is.na(records_breached)]

dt.frequency_bar_plot_total <- dt.frequency_bar_plot[,.N, by = list(breach_type_name)]
colnames(dt.frequency_bar_plot_total) <- c("breach_type_name", "total")
    
dt.frequency_bar_plot_count <- dt.frequency_bar_plot[,.N, by = list(breach_type_name, is_records_breached_na)]

dt.frequency_bar_plot_count <- merge(dt.frequency_bar_plot_count, dt.frequency_bar_plot_total, by = c("breach_type_name"))
dt.frequency_bar_plot_count[,percentage := N / total]
    
dt.order.this <- dt.frequency_bar_plot_count[is_records_breached_na == TRUE][order(percentage, decreasing = TRUE)]
    
dt.frequency_bar_plot$breach_type_name <- factor(dt.frequency_bar_plot$breach_type_name, 
                                                     levels = dt.order.this$breach_type_name)
    
ggplot(data = dt.frequency_bar_plot) + 
      geom_mosaic(aes(x = product(is_records_breached_na, breach_type_name), fill = is_records_breached_na)) +
      scale_fill_brewer(palette = "Set1") + theme_bw(base_size = 20) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
      ggtitle("Interaction between breach type \nand presence of records breached attribute") + xlab("Breach type") + ylab("Was Records Breached")

```



### Main analysis

#### Data cleaning process

While we were attempting to merge our raw CSV files to create our final data set, we ran into an issue where columns representing the same data across different CSV files had different names with inconsistent capitalization. We decided to standardize everything and made sure that the naming was consistent and in lower case. Afterward, we joined everything into a mega data table and subsetted to keep the columns relevant to our analysis. The most significant data cleaning process was formatting the reported dates. In the original *prcbreaches2005-18.csv* file, there wasn't a standardized date format nor a consistent pattern. We used a simple regex pattern detection function to segregate the rows into two distinct groups and then converted it to the standard R date format. Afterward, we checked to make sure no rows of data were dropped by comparing the numbers in our data set against the original data set. Afterward, we wrote the cleaned and formatted data table to a CSV file.

####Time series graph

```{r}
dt.time_series <- dt.data[,list(dt, breach_id)]
dt.time_series <- dt.time_series[order(dt)]
dt.year <- dt.time_series[,.N, by = year(dt)]
dt.year[,year := as.Date(paste0(year, "-01-01"))]
      
dt.plot <- ggplot(dt.year, aes(x = year, y = N)) + 
                  geom_bar(stat = "identity", fill = "lightblue") + 
                  theme_bw(base_size = 20) + 
                  xlab("Date") + ylab("Number of breach instances") + 
                  ggtitle("Number of data breaches from 2005 until 2018") +
                  scale_x_date(date_breaks = "1 year", date_labels = "%Y")
dt.plot
```

Since our dataset comprised of data breaches from 2005 to 2018, one of the first aspects we wanted to examine was how the frequency of breaches changes over time. To do so, we counted up the number of instances (rows) for each year and plotted the data using a bar chart. Over time, there seems to be fluctuations in the number of breach instances with some years experiencing much higher frequencies than others. However, the overall general trend is still an overall increase in number of breach instances over time. In lieu of ever-increasing amounts of data being produced by corporations and organizations and as a result, more opportunities for breaches to occur, this insight seems in line with what we would have speculated to be the trend.  

#### Distribution of Records_breached vs Category/Industry

```{r}
# Filtered by
# Have actual records breach data
# Number of instances for each category must be at least 5
# Remove outlier
dt.data_temp <- dt.data[!is.na(records_breached)]
cat_records_breached <- dt.data_temp[,list(records_breached, cat_name)]
    
# Count numbers of categories
dt.cat_count <- cat_records_breached[,.N, by = list(cat_name)]
dt.cat_count <- dt.cat_count[N >= 5]


cat_records_breached <- cat_records_breached[cat_name %in% dt.cat_count$cat_name]
cat_records_breached <- cat_records_breached[records_breached < 100000 & records_breached > 0]

vec_cat <- cat_records_breached[,list(med = median(records_breached)), by = list(cat_name)][order(med)]$cat_name
    
cat_records_breached$cat_name <- factor(x = cat_records_breached$cat_name, levels = vec_cat)


plt <- ggplot(data = cat_records_breached,aes(x = cat_name, y =records_breached)) + 
        geom_boxplot(fill = "lightblue") + 
        ggtitle("Distribution of Records Breached Across Category/Industry") + 
        ylab("Number of records breached") + xlab("") + scale_y_continuous(labels = scales::comma) + 
        theme_bw(base_size = 20) + coord_flip()

plt      
```

To obtain this distribution, we went ahead and plotted the data using boxplots. Boxplots are a beneficial way to visual the range and median, amongst other critical summary statistics. Plotting the boxplot using geom_boxplot() initially produced a chart where each distribution across industries was very small and unreadable due to very high outliers corresponding to massive data breaches. To address this challenge, we filtered the data to deal with rows with quantities of records breached <100,000 and records breached > 0. Furthermore, we also decided to look at industries with at least 5 breach instances (industry appeared at least five times in the dataset). The reason we decided to filter the data this way is so that we could focus on industries with frequent and impactful breach events. In addition,  Furthermore, since during the data cleaning step we converted many 'NA' to 0, we realized that including the 0 records breached values may skew our boxplot distribution. We also ignored instances with 'NA' for number of records breached. After doing so, we can see the distributions of records breached for each industry much more clearly. From this chart (Distribution of Records Breached Across Category/Industry), we can see that Medical Devices, Blood & Tissue, Analytics, Employer, and Researcher have the highest number of records breached when sorted by median number of records. 

#### Occurrences of Breaches within each Category/Industry


```{r}
dt.cat_freq <- dt.data[,.N, by = list(category)]
dt.cat_freq <- head(dt.cat_freq[order(N, decreasing = TRUE)], 5)
dt.cat_freq$category <- factor(dt.cat_freq$category, levels = dt.cat_freq$category)

ggplot(dt.cat_freq, aes(x = category, y = N), fill = "lightblue") +
  geom_bar(stat = "identity") + xlab("Category") + ylab("Occurences of Breaches") +
  theme_bw(base_size = 20) + ggtitle("How often breaches occur to specific category")

```


After looking at the distribution of number of records breached within each industry, we wanted to explore if the top 5 industries with the greatest median number of records breached were also the most targeted industries from a breach frequency perspective. To plot the frequency of breaches across industries, we chose to utilize a barplot. A barplot is great for this because it is immediately clear what the magnitude of each bar is as well as the relative magnitudes between the bars. We found that discharge data, payer(insurer), physician/hospital, public health, and employer were the top 5 categories that were impacted by data breaches. Comparing this chart to the previous visualization, Employer is the the only overlapping industry. We also observed that for certain industries such as Public Health, which has a history of high frequency of data breaches, the median quantity of data that was compromised for each breach was not necessarily very high. This means that just because industry A has more breach instances than industry B, does not imply that industry A will experience a greater magnitude of impact based on total number of records breached overall.

#### Breach_Type breakdown within Various Industries

```{r}
chr.industry <- "Payer (Insurer)"
dt.data_temp <- dt.data[!is.na(records_breached) & records_breached > 0]

dt.plot.this <- dt.data_temp[cat_name == chr.industry][,list(breach_type, breach_type_name, records_breached)]
dt.plot.this <- dt.plot.this[,list(records_breached = sum(records_breached)), by = list(breach_type, breach_type_name)]
dt.plot.this$breach_type <- factor(x = dt.plot.this$breach_type,
                                   levels = dt.plot.this[order(records_breached, decreasing = TRUE)]$breach_type)
dt.plot.this$breach_type_name <- factor(x = dt.plot.this$breach_type_name,
                                        levels = dt.plot.this[order(records_breached, decreasing = TRUE)]$breach_type_name)

# If the proportion of largest to smallest breach type instances is greater than 100, then use log scales
plt <- ggplot(dt.plot.this, aes(x = breach_type, 
                                y = records_breached, 
                                fill = breach_type_name,
                                text = paste0("Breach type name: ", breach_type_name,
                                              "\nRecords Breached: ", prettyNum(records_breached, scientific=FALSE, big.mark=",")))) + 
  geom_bar(stat = "identity") + theme_bw(base_size = 20) + 
  ggtitle(paste0("Total Records Breached for ", chr.industry, " by Breach Type")) + 
  xlab("Breach Type") + ylab("Total Records Breached") + scale_fill_colorblind() + scale_y_log10()
plt
```

We wanted to dive deeper and explore specific industries to understand the major reasons that data was compromised, which can be examined using the breach_type attribute. To do so, we broke the total records breached down for each industry by breach type. We went ahead and colored the bars various colors to represent the different breach types. Let us start by examining the Payer(Insurer) industry. For the Payer (Insurer) industry, we find that portable device loss (PORT), unintended disclosure (DISC), stationery device loss (STAT), Unknown (UNKN), and Hacking (HACK) were the top 5 breach_types experienced along with a few others. However, these breach types are not present in all industries. Depending on the context of the industry, the industry may be more susceptible to some breach types and not others. For instance, the Blood and Tissue industry only have four major breach types: 1) Insider Fraud (INSD), 2) Portable Device Loss (PORT), 3) Unintended Disclosure (DISC), and 4) Stationery Device Loss (STAT). 

#### Distribution of Records_breached Across Region

```{r}
dt.region <- dt.data[nchar(region) > 2][,list(dt, region, records_breached)]
dt.region <- dt.region[!is.na(records_breached) & records_breached > 0]
dt.region <- dt.region[region != "Puerto Rico"]
ggplot(data = dt.region[records_breached < 100000],
       aes(x = reorder(region,records_breached,FUN =median), y =records_breached)) + geom_boxplot() + 
  coord_flip() + ggtitle("Distribution of Records Breached Across Region") + 
  ylab("Number of Records Breached") + xlab("") + theme_bw(base_size = 20)
```

For this particular visualization, we faceted the number of records_breached by region. We turned to geom_box() to better visualize our distribution across regions. Prior to plotting the data, we left out the instances with 'NA' and focused on records_breached greater than 0 records and less than 50,000 records. Filtering the data down to this range helped us obtain a meaningful visualization where we can adequately observe the underlying distribution of number records breached values while minimizing the effects of potential skewness due to '0' records breached measures. When sorted by median number of records breached, there does not seem to be a huge discrepancy in the distribution of number of records breached based on region. However, it does seem like there are a fair number of data points scattered to the right of each boxplot. These points may represent the more massive data breaches, which occur here and there, but are not indicative of the more consistent pattern of breaches involving 20,000 records or less.   

#### Mosaic Plot of Region and Breach Type

```{r, fig.height = 10}
dt.breach_type_region <- dt.data[,list(breach_type_name, region)]
dt.breach_type_region <- dt.breach_type_region[nchar(region) > 2]
dt.breach_type_region <- dt.breach_type_region[!region %in% c("Puerto Rico", "Argentina")]

dt.breach_type_count <- dt.breach_type_region[,.N, by = list(region)]
dt.breach_type_count <- dt.breach_type_count[order(N, decreasing = TRUE)]

dt.breach_type_region$region <- factor(dt.breach_type_region$region,
                                       levels = c(dt.breach_type_count$region)) 


dt.breach_type_count2 <- dt.breach_type_region[,.N, by = list(breach_type_name)]
dt.breach_type_count2 <- dt.breach_type_count2[order(N, decreasing = TRUE)]

dt.breach_type_region$breach_type_name <- factor(dt.breach_type_region$breach_type_name,
                                       levels = c(dt.breach_type_count2$breach_type_name)) 

    
ggplot(data = dt.breach_type_region) + 
      geom_mosaic(aes(x = product(breach_type_name, region), fill = region)) +
      scale_fill_brewer(palette = "Set1") + theme_bw(base_size = 20) + 
      theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5)) +
      ggtitle("Interaction between breach type \nand region") + xlab("Region") + ylab("Breach type")
```

We wanted to explore if there was any interaction between the types of breaches and the region. Mosaic plots are great for finding these types of interactions because categories will be much larger or much smaller compared to the other columns if there is interaction. Looking at the chart, there is nothing that immediately jumps out to us. It does not seem that there is any significant interaction between region and breach type. However, we can also see that HACK, DISC, PHYS, PORT are the most common types of data breaches across each of the five regions.


#### Distribution of Records Breached Across States

```{r, fig.height = 12}
dt.states <- dt.data[nchar(state) > 2][,list(dt, state, records_breached)]
dt.states <- dt.states[!is.na(records_breached)]
ggplot(data = dt.states[records_breached < 100000 & records_breached > 0],
       aes(x = reorder(state,records_breached,FUN =median), y =records_breached)) + geom_boxplot() + 
  coord_flip() + ggtitle("Distribution of Records Breached Across States") + 
  ylab("Number of Records Breached") + xlab("") + theme_bw(base_size = 20)
```

For this visualization, we faceted the number of records breached by the state in which the breach occurred using boxplots. Once again, we filtered out the 'NA' values and focused on data with less than 100,000 and greater than 0 records breached per breach. Hawaii, Iowa, DC, West VA, and Georgia come out as the top 5 when sorted by median using boxplots. We also noticed that the distribution of data of many categories exhibited large spreads, which may allude to massive data breaches where an alarmingly high number of records were breached per breach. In particular, GA, AZ, and IN stand out as having particularly high variance in the number of records breached. Lastly, we noticed that there may be potential outliers to the right. However, since we subsetted the data earlier to limit it to instances with less than 100,000 number of records breached, we were able to disregard most of the more extreme and skewed values. 

#### Breach type by states

```{r}
dt.states <- dt.data[nchar(state) > 2][,list(dt, state, breach_id, breach_type_name)]

dt.map <- dt.states[,.N, by = list(state, breach_type_name)]
dt.map <- dt.map[state %in% state.name]
colnames(dt.map) <- c("state", "breach_type_name", "breach_instance")

dt.map_total <- dt.states[,.N, by = list(state)]
dt.map_total <- dt.map_total[state %in% state.name]
colnames(dt.map_total) <- c("state", "breach_instance")

dt.map$state <- factor(dt.map$state,levels = dt.map_total[order(breach_instance, decreasing = TRUE)]$state)

dt.map <- dt.map[state %in% head(dt.map_total[order(breach_instance, decreasing = TRUE)]$state, 5)]

ggplot(dt.map, aes(x = state, y = breach_instance, fill = breach_type_name)) + 
  geom_bar(stat = "identity", position = "dodge") + 
  theme_bw(base_size = 20) +
  xlab("State") + ylab("Breach Instance") + scale_fill_colorblind() +
  ggtitle("Breach type by state")
```

Next, we further examined which types of breaches are the most prevalent within the top 5 states with the highest number of breaches. We found that amongst the top 5 states, there seemed to be an overarching trend with hacking, unintended disclosure, and physical loss as the major reasons for breaches. Knowing this information, companies in these states can take preventative measures to secure their data against these potential threats. 

#### Breach source 

```{r, fig.height = 10}

dt.breach_source_count <- dt.data[,.N, by = list(breach_source)]
dt.breach_source_count$breach_source <- factor(dt.breach_source_count$breach_source,
                                               levels = dt.breach_source_count[order(N, decreasing = FALSE)]$breach_source)
ggplot(dt.breach_source_count, aes(x = breach_source, y = N)) + geom_bar(stat = "identity") + 
  xlab("Breach Source") + ylab("Frequency of Breach Source") + theme_bw(base_size = 20) + 
  ggtitle("Frequency of Source of Breaches") + coord_flip()

```

After a major data breach occurs, it is up to companies and organizations to decide which entity they would like to publicize this news. We were interested in seeing which entities tend to be the most frequently chosen. To further examine the source of breaches column, we used data.table to narrow down to only the breach_source column, do a group by on this column, and lastly run a frequency count of the number of instances in which a particular entity/source disseminated this breach information. The US Department of Health and Human Services, Media, California Attorney General, PHIPrivacy.net, and the Maryland Attorney General ranked as the top 5 most frequent sources of breaches. Having a US government entity and Media in the top 5 was somewhat expected because the public does receive a lot of breaking news via media outlets (ie. news, social media, etc.) or press releases from the government. However, we were surprised to see two states' attorney generals in the top 5. Upon further exploratory data analysis of the frequency of occurrence of data breaches across states, our visualizations confirmed that California and Maryland are in first and third place, respectively, for the highest frequency of data breaches. It is interesting that Texas, which was runner up for this data breach frequency across states visualization, and other states with high numbers of breaches did not have their attorney general featured in our breach source analysis. We speculate that there are certain domains that the attorney general of particular states will present on or not take on. Looking at the breach_type that the California and Maryland Attorney General will present on, it seems like they will publicize hack-related data breaches. Furthermore, it is possible that companies will choose to disclose the information through other outlets that may better help to preserve their image in light of massive data breaches. 

#### Breach instances by state population 

```{r}
# State population data
dt.states <- dt.data[nchar(state) > 2][,list(dt, state, breach_id)]
dt.map <- dt.states[,.N, by = state]
dt.map <- dt.map[state %in% state.name]
dt.merged_data <- merge(dt.state_population, dt.map, by = c("state"))
dt.merged_data[,population := population / 1000000]

plt <- ggplot(dt.merged_data, aes(x = population, y = N)) + geom_point() + 
          xlab("State Population (in millions)") + ylab("Number of Breach Instance") + theme_bw(base_size = 20) +
          ggtitle("Number of breach instances vs State Population")

plt
```

We wanted to see if there were any states that had an outsized number of breach attempts. We cannot look at just the net number of attempts because some states will have more companies than others. In order to provide a sense of scale for these metrics, we wanted to look at breach attempts scaled by state population (as a proxy for the number of companies in that state). We used data from the US Census Bureau to get the populations of each state. They are from Q3 of 2017. We plotted number of breach attempts by the population for each state. 

Looking at the relationship between population size and number of breaches, we see a clear positive linear relationship, which is what we would expect to find: as population size increases, the number of breaches increase as well. Other initial findings using time series visualizations also confirmed that the number of breach instances per year increases over time.This aligns with our expectations that as the amount of data being produced increases, entities are at greater risk for a potential compromise with their data. 

Upon examining the data from a regional perspective, we found that Maryland and California both experienced more breaches than expected. A potential reason for these outsized number of breaches for California and Maryland could be because California is a big tech hub with a huge population and Maryland is a big hub for health care technologies, which would lend for increased attempts at data breaching. 

#### Limitations, Future Directions, Lessons Learned

The first limitation we experienced involved missing data in our continuous data attribute, records_breached. This was a major inconvenience for us because this was of the only continuous data we had in our data set. Because of this, we were unable to obtain a better understanding of the proportion of data over total records within a corporation that was compromised. As a result, we simply analyzed the number of records breached. The second limitation was that our data set only contained ~5000 data breach instances which is not a lot of data to work with in the first place. During the plotting phase, we ignored instances with missing values, which further reduced the size of the data we had to work with. Lastly, the data source also only provided a very high level view of data breaches. Theoretically, a single data breach could compromise multiple entities which means that a more complete dataset should provide us with a hierarchical view of affected organizations.

In the future, we hope to conduct further analysis on the distribution of records breached across the 'entity' category, which encompasses a broad spectrum of industries per category. Although this category is rather broad and not as specific as the industry attribute we used in our analysis, doing so could provided further direction on what industries are most at risk at data breach. Additionally, we would have also liked to examine the types of topics that breach sources typically present on. Unfortunately, our data set did not include this information, but next steps would include gathering additional data to help us to complete this objective. Lastly, given that we had more data, we would have also liked to perform additional time series analysis and predictive modeling on the number of records breached given an anticipated data breach.

This project represents a culmination of the applied skills that we have taken away from the Exploratory and Data Analysis course. At a baseline, we learned to properly design our graphs for exploratory versus presentation purposes. On a deeper level, we learned to create a diversity of visualizations that helped to lend value to our analysis and were aesthetically pleasing. This aspect was challenging for us because we created so many graphs initially and wanted to include them all. In the end, we had to narrow down to the ones which were pertinent and that best complemented the story we wanted to present. Lastly, we believe the greatest takeaway for all of us was slowing down to understand the data and its context even before we attempted to plot it. This means reading up on what each attribute and understanding potential edge cases and outliers. Building this intuition beforehand set the foundation for a logical and streamlined flow when we did begin our data analysis and visualization.






